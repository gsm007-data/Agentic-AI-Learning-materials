{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Hugging Face API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Hugging Face?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A platform for pre-trained AI models (NLP, Vision, Audio).\n",
    "Provides an API for inference without needing GPUs.\n",
    "Model Hub: https://huggingface.co/models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Use Hugging Face API?\n",
    "\n",
    "✅ Access to thousands of models (GPT-2, BERT, T5, Whisper, etc.)\n",
    "\n",
    "✅ No need for model training or GPUs\n",
    "\n",
    "✅ Simple REST API for quick prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Hugging Face API Key\n",
    "\n",
    " - Steps\n",
    "    - Get API Key\n",
    "    - Go to Hugging Face → Settings → Access Tokens\n",
    "    - Click New Token, set permissions to \"Read\"\n",
    "    - Copy the token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (5.1.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from transformers) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from transformers) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from transformers) (0.23.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from transformers) (4.67.3)\n",
      "Requirement already satisfied: filelock in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (3.23.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2026.2.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: anyio in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.3.1)\n",
      "Requirement already satisfied: typer>=0.23.1 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from typer-slim->transformers) (0.23.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from typer>=0.23.1->typer-slim->transformers) (8.1.8)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from typer>=0.23.1->typer-slim->transformers) (13.9.4)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from typer>=0.23.1->typer-slim->transformers) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim->transformers) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim->transformers) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.1->typer-slim->transformers) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gourasundarmohanty/miniconda3/envs/py313/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"API key not found! Make sure you have a .env file with HUGGINGFACE_API_KEY.\")\n",
    "else:\n",
    "    print(\"API key loaded successfully!\")\n",
    "headers = {\"Authorization\": f\"Bearer {API_KEY}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type-1: Direct loading model into runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = (\n",
    "    \"Artificial intelligence (AI) refers to the simulation of human intelligence in machines \"\n",
    "    \"that are designed to think and learn.\"\n",
    ")\n",
    "\n",
    "ARTICLE = \"\"\"Generative AI (GenAI) is a transformative branch of artificial intelligence that creates new content—including text, images, code, and audio—by learning patterns from existing data. Utilizing technologies like Large Language Models (LLMs) and diffusion models, GenAI enables natural language interactions, automating complex tasks and enhancing creativity across industries. \n",
    "Key Aspects of Generative AI:\n",
    "How it Works: Generative AI models are trained on massive datasets to learn underlying structures and predict new data instances.\n",
    "Key Applications:\n",
    "Content Generation: Drafting articles, marketing copy, and creative writing.\n",
    "Design & Art: Creating images, videos, and 3D designs, such as using Stable Diffusion.\n",
    "Software Development: Coding, debugging, and software development, reducing manual effort.\n",
    "Personalization: Delivering customized, AI-driven experiences in e-commerce and customer service.\n",
    "Benefits & Impact: GenAI enhances efficiency, drives innovation, and acts as a collaborative tool for professional tasks.\n",
    "Risks & Challenges: Key concerns include \"hallucinations\" (generating inaccurate info), data privacy, inherent biases, and potential copyright issues. \n",
    "As GenAI matures, it is moving from a novelty tool to a core component in enterprise workflows, fundamentally altering how humans interact with technology. \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Inference API URL for summarization and generation\n",
    "SUMMARIZATION_MODEL_1 = \"facebook/bart-large-cnn\"\n",
    "SUMMARIZATION_MODEL_2 = \"sshleifer/distilbart-cnn-12-6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, model_name):\n",
    "    summarizer = pipeline(\"text-generation\", model=model_name)\n",
    "    summary = summarizer(text, max_length=20, min_length=5)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please make sure the generation config includes `forced_bos_token_id=0`. \n",
      "Loading weights: 100%|██████████| 316/316 [00:00<00:00, 426.49it/s, Materializing param=model.decoder.layers.11.self_attn_layer_norm.weight]   \n",
      "\u001b[1mBartForCausalLM LOAD REPORT\u001b[0m from: facebook/bart-large-cnn\n",
      "Key                                                       | Status     |  | \n",
      "----------------------------------------------------------+------------+--+-\n",
      "model.encoder.layers.{0...11}.final_layer_norm.bias       | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.self_attn_layer_norm.bias   | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.self_attn_layer_norm.weight | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.fc2.weight                  | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.self_attn.k_proj.weight     | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.self_attn.v_proj.bias       | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.self_attn.q_proj.bias       | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.fc2.bias                    | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.fc1.weight                  | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.self_attn.v_proj.weight     | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.self_attn.q_proj.weight     | UNEXPECTED |  | \n",
      "model.encoder.layernorm_embedding.bias                    | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.self_attn.out_proj.bias     | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.fc1.bias                    | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.self_attn.out_proj.weight   | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.self_attn.k_proj.bias       | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.final_layer_norm.weight     | UNEXPECTED |  | \n",
      "model.encoder.layernorm_embedding.weight                  | UNEXPECTED |  | \n",
      "model.encoder.embed_positions.weight                      | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "Passing `generation_config` together with generation-related arguments=({'min_length', 'max_length'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: [{'generated_text': 'Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are designed to think and learn.'}]\n"
     ]
    }
   ],
   "source": [
    "summary = summarize_text(input_text, SUMMARIZATION_MODEL_1)\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarize_text(input_text, \"philschmid/bart-large-cnn-samsum\")\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type-2: Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = (\n",
    "    \"Artificial intelligence (AI) refers to the simulation of human intelligence in machines \"\n",
    "    \"that are designed to think and learn.\"\n",
    ")\n",
    "\n",
    "# Hugging Face Inference API URL for summarization and generation\n",
    "SUMMARIZATION_MODEL_1 = \"facebook/bart-large-cnn\"\n",
    "SUMMARIZATION_MODEL_2 = \"sshleifer/distilbart-cnn-12-6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are designed to think and learn. AI is a form of computer science that aims to simulate human intelligence. It is used to create machines that can think, learn and act on their own.\n"
     ]
    }
   ],
   "source": [
    "def summarize_text_1(text):\n",
    "    api_url = f\"https://router.huggingface.co/hf-inference/models/{SUMMARIZATION_MODEL_1}\"\n",
    "    payload = {\"inputs\": text}\n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "    summary = response.json()[0]['summary_text']\n",
    "    return summary\n",
    "summary = summarize_text_1(input_text)\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Named Entity Recognition (NER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 576.58it/s, Materializing param=classifier.weight]                                      \n",
      "\u001b[1mBertForTokenClassification LOAD REPORT\u001b[0m from: dslim/bert-base-NER\n",
      "Key                      | Status     |  | \n",
      "-------------------------+------------+--+-\n",
      "bert.pooler.dense.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.bias   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': np.float32(0.9895768), 'index': 1, 'word': 'El', 'start': 0, 'end': 2}, {'entity': 'B-PER', 'score': np.float32(0.8199849), 'index': 2, 'word': '##on', 'start': 2, 'end': 4}, {'entity': 'I-PER', 'score': np.float32(0.9979075), 'index': 3, 'word': 'Mu', 'start': 5, 'end': 7}, {'entity': 'I-PER', 'score': np.float32(0.96267045), 'index': 4, 'word': '##sk', 'start': 7, 'end': 9}, {'entity': 'B-ORG', 'score': np.float32(0.9987417), 'index': 9, 'word': 'Te', 'start': 24, 'end': 26}, {'entity': 'I-ORG', 'score': np.float32(0.9958287), 'index': 10, 'word': '##sla', 'start': 26, 'end': 29}, {'entity': 'I-ORG', 'score': np.float32(0.7535397), 'index': 11, 'word': 'and', 'start': 30, 'end': 33}, {'entity': 'B-ORG', 'score': np.float32(0.9826819), 'index': 12, 'word': 'Space', 'start': 34, 'end': 39}, {'entity': 'I-ORG', 'score': np.float32(0.99901974), 'index': 13, 'word': '##X', 'start': 39, 'end': 40}, {'entity': 'B-LOC', 'score': np.float32(0.9995152), 'index': 18, 'word': 'USA', 'start': 55, 'end': 58}]\n"
     ]
    }
   ],
   "source": [
    "# Loading model\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"Elon Musk is the CEO of Tesla and SpaceX, based in the USA.\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: {'entity_group': 'PER', 'score': 0.999356, 'word': 'Elon Musk', 'start': 0, 'end': 9}\n"
     ]
    }
   ],
   "source": [
    "# Inferencing model\n",
    "def named_entity_recognition(text):\n",
    "    API_URL = \"https://router.huggingface.co/hf-inference/models/dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "    payload = {\"inputs\": text}\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()[0]#[\"entity_group\"]\n",
    "\n",
    "print(\"Named Entities:\", named_entity_recognition(\"Elon Musk is the CEO of Tesla and SpaceX, based in the USA.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Question Answering (Roberta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 699.11it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertModel LOAD REPORT\u001b[0m from: distilbert-base-cased-distilled-squad\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "qa_outputs.bias   | UNEXPECTED |  | \n",
      "qa_outputs.weight | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "\n",
    "question, text = \"Who is Elon Musk?\", \"ELON MUSK IS THE CEO OF TESLA\"\n",
    "\n",
    "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[ 1.1711, -0.5700,  1.2161,  ..., -0.6587, -0.1305, -0.5470],\n",
      "         [ 1.3225, -0.6659,  1.7093,  ..., -0.7418, -0.3244, -0.6318],\n",
      "         [ 1.8805, -0.5053,  1.4130,  ..., -0.5677,  0.1197, -0.3915],\n",
      "         ...,\n",
      "         [ 1.3974,  0.3189,  1.4370,  ..., -0.3530,  0.0278,  0.2146],\n",
      "         [ 1.0326, -0.4906,  1.1371,  ..., -0.3295, -0.8474, -0.2820],\n",
      "         [ 0.9740, -0.8145,  1.0698,  ..., -1.0709,  0.8596, -0.2683]]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 102/102 [00:00<00:00, 672.74it/s, Materializing param=qa_outputs.weight]                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: CEO\n",
      "Score: 0.895262598991394\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "qa_result = qa_pipeline(question=question, context=text)\n",
    "print(\"Answer:\", qa_result['answer'])\n",
    "print(\"Score:\", qa_result['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: {'score': 0.9522801041603088, 'start': 46, 'end': 87, 'answer': 'natural language processing and AI models'}\n"
     ]
    }
   ],
   "source": [
    "def question_answering(question, context):\n",
    "    API_URL = \"https://router.huggingface.co/hf-inference/models/deepset/roberta-base-squad2\"\n",
    "    payload = {\"inputs\": {\"question\": question, \"context\": context}}\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "context_text = \"Hugging Face is a company that specializes in natural language processing and AI models. It provides a platform for model training and deployment.\"\n",
    "question_text = \"What does Hugging Face specialize in?\"\n",
    "print(\"Answer:\", question_answering(question_text, context_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimenting with Different Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How Find Model on Hugging Face Hub?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to Hugging Face Model Hub. https://huggingface.co/models\n",
    "Search for the model you want (e.g., GPT-Neo, Whisper, T5).\n",
    "Click on the model name to open its page.\n",
    "Copy the Model ID (it’s usually in the format organization/model-name)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Construct API URL\n",
    "The general format for Hugging Face inference API calls is:\n",
    "\n",
    "ttps://router.huggingface.co/hf-inference/models/models/{MODEL_ID}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"API key not found! Make sure you have a .env file with HUGGINGFACE_API_KEY.\")\n",
    "else:\n",
    "    print(\"API key loaded successfully!\")\n",
    "headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized Text: AI is revolutionizing industries by automating tasks and improving efficiency. It can also be used to create new products and services to improve quality of life and reduce costs. For more information on how to use AI in your business, visit CNN.com/ArtificialIntelligence.\n"
     ]
    }
   ],
   "source": [
    "API_URL = \"https://router.huggingface.co/hf-inference/models/facebook/bart-large-cnn\"\n",
    "\n",
    "payload = {\"inputs\": \"AI is revolutionizing industries by automating tasks and improving efficiency. It can also be used to create new products and services to improve quality of life and reduce costs. For more information on how to use AI in your business, visit CNN.com/ArtificialIntelligence.\"}\n",
    "\n",
    "response = requests.post(API_URL, headers=headers, json=payload)\n",
    "print(\"Summarized Text:\", response.json()[0][\"summary_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Models Available: 1000\n",
      "zai-org/GLM-5\n",
      "MiniMaxAI/MiniMax-M2.5\n",
      "openbmb/MiniCPM-SALA\n",
      "moonshotai/Kimi-K2.5\n",
      "Qwen/Qwen3-Coder-Next\n",
      "Nanbeige/Nanbeige4.1-3B\n",
      "zai-org/GLM-OCR\n",
      "openbmb/MiniCPM-o-4_5\n",
      "inclusionAI/Ming-flash-omni-2.0\n",
      "mistralai/Voxtral-Mini-4B-Realtime-2602\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"https://huggingface.co/api/models?full=true\")\n",
    "models = response.json()\n",
    "print(\"Total Models Available:\", len(models))\n",
    "\n",
    "# Print the first 5 model names\n",
    "for model in models[:10]:\n",
    "    print(model[\"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
